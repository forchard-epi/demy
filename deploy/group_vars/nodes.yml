#general
ansible_become_pass: "{{lookup('passwordstore', 'infra/bgdt')}}"
decrypt_disk_pass: "{{lookup('passwordstore', 'infra/bgdt-disk')}}"
proxy_host: proxy.admin2.oxa.tld
proxy_port: 3128 
cran_mirror: http://cran.irsn.fr
apache_mirror: http://mirrors.ircam.fr
back_network: prostore
front_network: proback
java_home: /usr/lib/jvm/java-8-openjdk-amd64
java_home_spark: /usr/lib/jvm/java-8-openjdk-amd64
java_package: openjdk-8-jdk
java_package_spark: openjdk-8-jdk
java_default: java-1.8.0-openjdk-amd64
protobuf_ver: 2.5.0
hadoopusers: [hadoop, spark, hive, hdfs, yarn, mapred, airflow]
hadoopusers_ssh: [airflow, spark]
crypted_disk: space

#admin public keys:
admins:
- "ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDZk3latkv/+SVInPZpEicwoQ3huM2cJXfrdrjB25PQ/4flJM27j+f/I4XJM4I87KNwDJRiOWH0POVIcYLoMeVVR5PHM5rSxVe6bfcn1cJSgEMms/vLJNGpXn9HvK13YEXgBSKj/3u6s3+E6MrxKNDIX/hEfro5zrFzRqYDH25FrkeFyuW1CB2B7pLnTGdHOb+PTY/2/gQzVjteORlcg/XsPq1AdzZfHpwoDOXGVN0HtGjQYQ6B8un6YzgI9yEJdiEfqR8etYAD5cLoQKMLnObEaXSI2KwrGQn8cOmKuRYmQjyFyeFx4+KLnlnq1Upkv6F/UVh8vJJ2IFn+wCeQaqE1 fod@bastillito"
- "ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDEmqgzYKiOkqvDtQbMwD3JYv4CuYggFbgW8igaxteJFkFDOmyK4TfsjkOQywvHod9yO3XJuvDxT7XOdhfQXzHfXibgmG/5iKfiBmrp8qbL6AnmE9dkcSjOhW+PentbhgdUrmcGSpB0esc67EU38tKw/224FmpLmjyQ5yf+U0wx6HQ3VJcnky98pasQ1WCEyGzfT4LxlqjYTx1oVoDem0x+cURRIYOXu0u8E2v1BUP8BW/KxBcP6Gnu0sP63YwYWVc6XlU9AX/gyn5Dpp+Qxst6b3aAxjQFfFzoWc+jdF1fncQnVduzgQqyVpQXlYmxjyPzuBQ+78mvfaQLr/3O56Sp adrian@adrian-HP-ZBook-15u-G4"

#Hadoop
hadoop_cluster_name: "demy"
hadoop_ver: 3.1.0
hadoop_install: "/{{ crypted_disk  }}/hadoop"
hadoop_home: "{{ hadoop_install }}/hadoop_home"
hadoop_run: "{{ hadoop_install }}/hadoop_run"
hadoop_log_dir: "{{ hadoop_run }}/logs/hadoop"
hadoop_conf_dir: "{{ hadoop_home }}/etc/hadoop"
hadoop_pid_dir: "{{ hadoop_run }}/PID"

#dependencies durectories
tmp_dir: "/{{ crypted_disk  }}/tmp"
custom_libs:  "/{{ hadoop_install }}/lib"

#hdfs
hdfs_port: 8020
hdfs_file_buffer_bytes: "{{ 128*1024 }}"
hdfs_tmp_dir: "hdfs:///tmp"

#Python
python_ver: 3.6.7
python_home: "{{ hadoop_install }}/lib/python"
python: "{{ python_home }}/bin/python3"
pip: "{{ python_home }}/bin/pip3"

#Spark
spark_ver: 2.3.2
spark_home: "{{ hadoop_install }}/spark_home"
spark_run: "{{ hadoop_install }}/spark_run"
spark_thrift_port: "10000"
spark_thrift_driver_mem_mb: "512"
spark_thrift_driver_cores: "1"
spark_thrift_executor_mem_mb: "1536"
spark_thrift_executor_cores: "2"
spark_dynamic_allocation: true
spark_r_packages_url: ["https://cran.r-project.org/src/contrib/Archive/rgdal/rgdal_1.2-20.tar.gz"]
spark_r_packages_url_name: ["rgdal"]
spark_r_packages: ["classInt", "colorRamps", "devtools", "jsonlite", "Rcpp", "dplyr", "tidyr", "forecast", "ISOweek", "lubridate", "data.table", "plyr", "fBasics", "Hmisc", "ggplot2", "RColorBrewer", "colorRamps", "gridExtra", "scales", "lattice", "xtable", "mapproj", "maptools", "MASS", "sp", "rgeos", "raster", "surveillance", "epiR", "timeSeries", "stringr", "timeDate", "msm", "tseries", "xts", "rJava", "xlsx", "base64enc", "grDevices", "acepack", "bitops", "caTools", "curl", "Formula", "graphicsQC", "Hmisc", "httr", "latticeExtra", "magrittr", "memoise", "pixmap", "whisker", "withr", "XML", "blockrand", "randomizeR", "GADMTools", "sparklyr", "roxygen2", "testthat"]
spark_mysql_jdbc_version: 5.1.39

#Zeppelin
zeppelin_version: "v0.8.0" #"a2473daf8e7400fb1cb635ba97f66835cad58ffc"
zeppelin_home: "{{ hadoop_install }}/zeppelin_home"
zeppelin_run: "{{ hadoop_install }}/zeppelin_run"
zeppelin_port: "8080"
zeppelin_https_enabled: "true"
zeppelin_https_cert_auth_enabled: "false"
zeppelin_https_port: "8443"
zeppelin_https_bundle_cert: "/space/etc/ssl/wildcard.voozanoo.net.bundle.crt"
zeppelin_https_private_key: "/space/etc/ssl/wildcard.voozanoo.net.key"
zeppelin_keystore_pass: "{{lookup('passwordstore', 'bgdt-zeppelin-kstore')}}"
zeppelin_driver_memory_mb: "1024"
zeppelin_driver_cores: "1"
zeppelin_executor_memory_mb: "2000"
zeppelin_executor_cores: "3"
zeppelin_anonymous_enabled: false
zeppelin_shiro_config:
- [users,"fod=","fod={{lookup('passwordstore', 'zeppelin-pass-fod')}},admin"]
- [users,"stage=","stage={{lookup('passwordstore', 'zeppelin-pass-stage')}},contributor"]
- [users,"jp=","jp={{lookup('passwordstore', 'zeppelin-pass-jp')}},contributor"]
- [users,"thomas=","thomas={{lookup('passwordstore', 'zeppelin-pass-thomas')}},contributor"]
- [users,"pascal=","pascal={{lookup('passwordstore', 'zeppelin-pass-pascal')}},contributor"]
- [users,"adrian=","adrian={{lookup('passwordstore', 'zeppelin-pass-adrian')}},admin"]
- [main,"shiro.loginUrl =", "shiro.loginUrl = /api/login"]
- [main,"securityManager.sessionManager.globalSessionTimeout =", "securityManager.sessionManager.globalSessionTimeout = {{ 1000 * 3600 * 24 }}"]
- [main,"securityManager.sessionManager =", "securityManager.sessionManager = $sessionManager"]
- [main,"sessionManager =", "sessionManager = org.apache.shiro.web.session.mgt.DefaultWebSessionManager"]
- [roles,"admin =","admin = *"]
- [roles,"contributor =", "contributor = *"]
- [urls,"/\\*\\* =","/** = authc"]
- [urls,"/api/interpreter/\\*\\* =", "/api/interpreter/** = authc, roles[admin]"]
- [urls,"/api/configurations/\\*\\* =", "/api/configurations/** = authc, roles[admin]"]
- [urls,"/api/credential/\\*\\* =", "/api/credential/** = authc, roles[admin]"]
zeppelin_maven_repository: "http://central.maven.org/maven2/"
zeppelin_spark_packages: 
- "org.jsoup:jsoup:1.11.2"
- "org.apache.lucene:lucene-core:7.2.1"
- "org.apache.lucene:lucene-queryparser:7.2.1"
- "org.apache.lucene:lucene-analyzers-common:7.2.1"
- "org.apache.httpcomponents:httpmime:4.5.6"
- "com.databricks:spark-xml_2.11:0.4.1"
- "org.apache.commons:commons-text:1.4"
- "ml.dmlc:xgboost4j-spark:0.80"
- "{{ zeppelin_run }}/custom-libs/demy-machine-learning-library_2.11-1.0.jar"
- "{{ zeppelin_run }}/custom-libs/demy-storage-layer_2.11-1.0.jar"
- "{{ zeppelin_run }}/custom-libs/demy-core_2.11-1.0.jar"
- "{{ zeppelin_run }}/custom-libs/demy-twitter-track-assembly-1.0.jar"

#zeppelin_spark_jars: "{{ zeppelin_run }}/custom-libs/demy-machine-learning-library_2.11-1.0.jar"
zeppelin_spark_perNoteIsolation: "shared" #isolated, shared or scoped
zeppelin_spark_perUserIsolation: "scoped" #isolated, shared or scoped
zeppelin_spark_master: "yarn-client" 

#Head Node
host_keys_to_insert: []
hdfs_namenode_dir: "{{ hadoop_run }}/head_node"
hdfs_hosts: "{{ hadoop_conf_dir }}/datanodes"
hdfs_hosts_exclude: "{{ hadoop_conf_dir }}/datanodes-excluded"
hdfs_blocksize_bytes: "{{ 64 *1024*1024 }}"
hdfs_namenode_thread_count: 64
hdfs_default_replication: 3 
hdfs_namenode_daemon_memory_mb: 1024

#Data node
hdfs_datanode_dir: "{{ hadoop_run}}/data"
hdfs_datanode_daemon_memory_mb: 512
hdfs_datanode_ipc_port: 50020

#Httpsfs node
hdfs_http_port: 8081
hdfs_http_admin_port: 8082

#yarn
yarn_log_dir: "{{ hadoop_run }}/logs/yarn"

#Resource Manager
yarn_acl: false  
yarn_admin_acl: "*"
yarn_log_aggregation: false 
yarn_resourcemanager_port: "8032" 
yarn_resourcescheduler_port: "8030"  
yarn_resourcetracker_port: "8031"
yarn_resourcemanager_recovery: true
yarn_resourcemanager_store_class: "org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore"
yarn_resourcemanager_store_uri: "file://{{ hadoop_run }}/resource_manager/store"
yarn_admin_port: "8033"
yarn_webapp_port: "8090" 
yarn_webproxy_port: "8091" 
yarn_webproxy_daemon_memory_mb: 512
yarn_scheduler: "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler" 
yarn_scheduler_capacity_calculator: "org.apache.hadoop.yarn.util.resource.DominantResourceCalculator"
yarn_resourcemanager_daemon_memory_mb: 1024
yarn_container_min_mb: 200 
yarn_container_max_mb: "{{ 30 * 1024 }}"
yarn_container_max_cores: 4
yarn_hosts: "{{ hadoop_conf_dir }}/nodemanagers"
yarn_hosts_exclude: "{{ hadoop_conf_dir }}/nodemanagers-excluded" 
yarn_applicationmanager_max_percent: "0.3"


#Node managers
yarn_nodemanager_check_vmem: false
yarn_nodemanager_check_pmem: true
yarn_nodemanager_vmem_pmem_ratio: 4.1
yarn_nodemanager_local_dirs: "{{ hadoop_run }}/yarn-nm"
yarn_nodemanager_log_seconds: "{{ 3600 * 24 }}"
yarn_nodemanager_remote_log_dir: "/logs/yarn" 
yarn_nodemanager_remote_log_dir_suffix: "logs" 
yarn_nodemanager_detect_hardware: true
yarn_nodemanager_daemon_memory_mb: "512"
#yarn_nodemanager_memory_mb: "3072"
yarn_nodemanager_system_reserved_memory_mb: "512"
yarn_nodemanager_resource_vcores: "-1"
yarn_nodemanager_logical_procs_as_cores: true
yarn_nodemanager_resource_pcores_multiplier: 1

#Mapreduce config
mapreduce_framework_name: "yarn"
mapreduce_task_io_sort_factor: 100 
mapreduce_reduce_shuffle_parallelcopies: 50  
mapreduce_jobhistory_port: 10020
mapreduce_jobhistory_webapp_port: 19888 
mapreduce_jobhistory_intermediate_done_dir: "/mr-history/tmp" 
mapreduce_jobhistory_done_dir: "/mr-history/done"
mapreduce_jobhistory_memory_mb: 512

#Custom service ips
demy_custom_hosts:
- {ip: "10.1.4.93", name: "epifiles.voozanoo.net"}
- {ip: "10.1.16.125", name: "epifiles.preprod.voozanoo.net"}
     

#mail
smtp_host: "smtp.admin2.oxa.tld"
smtp_port: 25
smtp_starttls: False
smtp_ssl: False
smtp_user: ""
smtp_password: ""

#airflow
af_version: "1.10.1"
af_home: "{{ hadoop_install }}/airflow_home"
af_run: "{{ hadoop_install }}/airflow_run"
af_database: "{{ hadoop_install }}/database"
af_timezone: "utc"
af_max_active_tasks: 32
af_port: 8444
af_ssl_cert: "/space/etc/ssl/wildcard.voozanoo.net.crt"
af_ssl_key: "/space/etc/ssl/wildcard.voozanoo.net.key"
af_mailfrom: "datascience@epiconcept.fr"
af_max_threads: 2
